{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Romantic Poem Generator using GPT-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.5.1\n",
      "CUDA available: True\n",
      "CUDA device: NVIDIA GeForce RTX 4080 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import torch\n",
    "import re\n",
    "from transformers import (\n",
    "    GPT2LMHeadModel,\n",
    "    GPT2Tokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "from datasets import Dataset\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Prepare Pre-trained GPT-2 Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initially, we aim to create a system based on pre-trained model, GPT-2, to generate poems with general topic. Generative models are powerful at text generation and we want to learn what a classical model would perform now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-trained model: gpt2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02b4b8c106de49a699aafc2191a974c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f37c088cc1d9450e89ffa616bf659dac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af2ca7bbf72f46d7a2cb76234a1a3128",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1f84a9a01c64079a6a2b6a70e06e27d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91124f16967d4ac9b9e55d88fb0e05c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db7f6641170f48aca8365535abbcdbbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80169b9663b54ce480a27a7ca97f8a72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n",
      "Model parameters: 124,439,808\n",
      "Vocabulary size: 50257\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained GPT-2 model and tokenizer\n",
    "model_name = 'gpt2'  \n",
    "\n",
    "print(f\"Loading pre-trained model: {model_name}\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "# Set pad token \n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.config.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "print(f\"Model loaded successfully!\")\n",
    "print(f\"Model parameters: {model.num_parameters():,}\")\n",
    "print(f\"Vocabulary size: {len(tokenizer)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load Romantic Poems and Prepare Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We prepared an inspiring set containing romantic poems to fine tune the model for poem generation on [Kaggle](https://www.kaggle.com/datasets/michaelarman/poemsdataset?resource=download). They are text files and each line has similar length but does not always stand for a sentence. An example is shown below:\n",
    "```\n",
    "Fun of passionate enjoyment of romantic pleasure between\n",
    "Two beautifully attractive souls is a great fortune for\n",
    "Them to experience in the life of human world quite rare!\n",
    "It's a great blessing only rare personalities of good heart\n",
    "Have in this world full of doubts, taboos and conventions\n",
    "That prevent two good hearts to live in romantic enjoyment!\n",
    "Loving life so in romantic pleasure in fun for a brief period\n",
    "Or a longtime sure here is not in one's hand, but if that\n",
    "Happens naturally, it is a great boon bestowed on them by God!\n",
    "The creator of humans decrees all live according to His will;\n",
    "Love, romance, pleasure and peace are not for all to have in\n",
    "Life except by the chosen few who have had deep faith in God!\n",
    "For the ones who are in love of the Universal spiritual energy,\n",
    "All in world and Nature, perhaps God bestows such a boon indeed!\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 100 poem files in 'romantic'\n",
      "\n",
      "Loaded 100 poems successfully!\n",
      "\n",
      "Example poem (first 300 characters):\n",
      "Fun of passionate enjoyment of romantic pleasure between\n",
      "Two beautifully attractive souls is a great fortune for\n",
      "Them to experience in the life of human world quite rare!\n",
      "It's a great blessing only rare personalities of good heart\n",
      "Have in this world full of doubts, taboos and conventions\n",
      "That preven\n"
     ]
    }
   ],
   "source": [
    "# Load poems from the romantic folder\n",
    "def load_poems_from_folder(folder_path):\n",
    "    \"\"\"\n",
    "    Load all poems from text files in the specified folder.\n",
    "    Expected filename format: RomanticPoems*.txt\n",
    "    \"\"\"\n",
    "    poems = []\n",
    "    pattern = os.path.join(folder_path, \"RomanticPoems*.txt\")\n",
    "    files = glob.glob(pattern)\n",
    "    \n",
    "    print(f\"Found {len(files)} poem files in '{folder_path}'\")\n",
    "    \n",
    "    for file_path in files:\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                content = f.read().strip()\n",
    "                if content:  # Only add non-empty poems\n",
    "                    poems.append(content)\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {file_path}: {e}\")\n",
    "    \n",
    "    return poems\n",
    "\n",
    "# Load the poems\n",
    "POEMS_FOLDER = \"romantic\"  # Adjust this path if needed\n",
    "poems = load_poems_from_folder(POEMS_FOLDER)\n",
    "\n",
    "print(f\"\\nLoaded {len(poems)} poems successfully!\")\n",
    "print(f\"\\nExample poem (first 300 characters):\")\n",
    "print(poems[0][:300] if poems else \"No poems found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset prepared with 100 examples\n",
      "Example tokenized length: 512 tokens\n"
     ]
    }
   ],
   "source": [
    "# Prepare dataset for training\n",
    "def prepare_dataset(poems, tokenizer, max_length=512):\n",
    "    \"\"\"\n",
    "    Tokenize poems and prepare for training.\n",
    "    \"\"\"\n",
    "    # Add special tokens to mark beginning and end of poems\n",
    "    formatted_poems = [f\"<|startoftext|>{poem}<|endoftext|>\" for poem in poems]\n",
    "    \n",
    "    # Tokenize\n",
    "    encodings = tokenizer(\n",
    "        formatted_poems,\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        padding='max_length',\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    # Create dataset\n",
    "    dataset_dict = {\n",
    "        'input_ids': encodings['input_ids'].tolist(),\n",
    "        'attention_mask': encodings['attention_mask'].tolist()\n",
    "    }\n",
    "    \n",
    "    dataset = Dataset.from_dict(dataset_dict)\n",
    "    return dataset\n",
    "\n",
    "# Prepare the dataset\n",
    "train_dataset = prepare_dataset(poems, tokenizer)\n",
    "print(f\"Dataset prepared with {len(train_dataset)} examples\")\n",
    "print(f\"Example tokenized length: {len(train_dataset[0]['input_ids'])} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2.5: Fine-tune the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fine-tuned model is the poem generator we aim for. After fine-tuning, GPT-2 model would shift its weights onto the given inspiring set and its vocabulary, paragraph structure thus being better at poem generation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainer initialized. Ready to fine-tune!\n"
     ]
    }
   ],
   "source": [
    "# Set up training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./romantic_gpt2_output\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=400,  \n",
    "    per_device_train_batch_size=4,  \n",
    "    save_steps=100,\n",
    "    save_total_limit=2,\n",
    "    logging_steps=100,\n",
    "    learning_rate=1e-5,\n",
    "    warmup_steps=100,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    prediction_loss_only=True,\n",
    "    fp16=torch.cuda.is_available(),  \n",
    ")\n",
    "\n",
    "# Data collator for language modeling\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False  \n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    ")\n",
    "\n",
    "print(\"Trainer initialized. Ready to fine-tune!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting fine-tuning...\n",
      "This may take a while depending on your hardware and dataset size.\n",
      "==================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10000' max='10000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10000/10000 27:47, Epoch 400/400]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.623900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.570200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>2.448800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>2.304300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.142100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>2.025000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>1.884600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.751200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>1.597200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.477800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>1.350500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>1.242400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>1.122400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>1.030900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.932400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.846400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.765500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.688100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.624300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.572000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.515900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.472500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.432000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.393700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.360400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.336000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.303500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.282400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>0.260800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.244500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>0.224200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.210100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>0.199300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.187100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.177300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.167500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>0.159900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>0.147500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3900</td>\n",
       "      <td>0.144300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.137100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4100</td>\n",
       "      <td>0.130600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>0.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4300</td>\n",
       "      <td>0.122700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>0.114400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.113500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>0.108900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4700</td>\n",
       "      <td>0.104800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>0.102800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4900</td>\n",
       "      <td>0.098500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.096000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5100</td>\n",
       "      <td>0.095600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>0.089900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5300</td>\n",
       "      <td>0.090400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>0.088200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.084400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>0.082300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5700</td>\n",
       "      <td>0.081000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5800</td>\n",
       "      <td>0.078600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5900</td>\n",
       "      <td>0.079000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.074800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6100</td>\n",
       "      <td>0.074400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6200</td>\n",
       "      <td>0.073700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6300</td>\n",
       "      <td>0.071500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6400</td>\n",
       "      <td>0.071200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.070000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6600</td>\n",
       "      <td>0.068700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6700</td>\n",
       "      <td>0.069100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6800</td>\n",
       "      <td>0.068200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6900</td>\n",
       "      <td>0.066600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.066700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7100</td>\n",
       "      <td>0.064700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7200</td>\n",
       "      <td>0.063300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7300</td>\n",
       "      <td>0.063500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7400</td>\n",
       "      <td>0.061400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.062700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7600</td>\n",
       "      <td>0.060500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7700</td>\n",
       "      <td>0.059900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7800</td>\n",
       "      <td>0.059400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7900</td>\n",
       "      <td>0.059600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.059200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8100</td>\n",
       "      <td>0.059100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8200</td>\n",
       "      <td>0.059100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8300</td>\n",
       "      <td>0.060500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8400</td>\n",
       "      <td>0.057200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.058000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8600</td>\n",
       "      <td>0.057000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8700</td>\n",
       "      <td>0.057000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8800</td>\n",
       "      <td>0.056500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8900</td>\n",
       "      <td>0.055700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.056700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9100</td>\n",
       "      <td>0.056000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9200</td>\n",
       "      <td>0.056000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9300</td>\n",
       "      <td>0.055200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9400</td>\n",
       "      <td>0.056400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.054900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9600</td>\n",
       "      <td>0.055400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9700</td>\n",
       "      <td>0.054400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9800</td>\n",
       "      <td>0.053400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9900</td>\n",
       "      <td>0.054300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.054200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Fine-tuning complete!\n"
     ]
    }
   ],
   "source": [
    "# Fine-tune the model\n",
    "print(\"Starting fine-tuning...\")\n",
    "print(\"This may take a while depending on your hardware and dataset size.\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Fine-tuning complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model converges after approximately 8800 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to './romantic_gpt2_finetuned'\n"
     ]
    }
   ],
   "source": [
    "# Save the fine-tuned model\n",
    "model.save_pretrained(\"./romantic_gpt2_finetuned\")\n",
    "tokenizer.save_pretrained(\"./romantic_gpt2_finetuned\")\n",
    "print(\"Model saved to './romantic_gpt2_finetuned'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Generate Poems \n",
    "\n",
    "Now let's generate poems. The `generate_poem` function orchestrates the text generation process by first placing the model into **evaluation mode** and preprocessing the input `prompt` with a specific `<|startoftext|>` token to signal the beginning of a sequence. After converting this text into numerical tensors and moving them to the appropriate hardware device, the function utilizes `model.generate` within a memory-efficient `torch.no_grad()` context to predict the subsequent text. This generation step relies on **sampling parameters**—specifically `temperature`, `top_k`, and `top_p`—to introduce controlled randomness, ensuring the poem is creative rather than repetitive. Finally, the function iterates through the generated sequences, decodes the numerical tokens back into human-readable strings, and strips away the special structural tokens to return the clean, finished poem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopping Criteria\n",
    "There are two stopping criteria: the **hard criterion** (`max_length`), which limits the output to 200 **tokens**, and the **soft criterion**, which is the `<|endoftext|>` token generated by the model itself. These two combined offer a complete poem when the model finishes its thought naturally, or a safeguard that stops generation if it exceeds the maximum limit, preventing infinite loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_poem(model, tokenizer, prompt=\"\", max_length=200, temperature=0.9, \n",
    "                  top_k=50, top_p=0.95, num_return_sequences=1):\n",
    "    \"\"\"\n",
    "    Generate a poem using the fine-tuned model.\n",
    "    \n",
    "    Args:\n",
    "        prompt: Starting text for the poem (empty for free generation)\n",
    "        max_length: Maximum length of generated text\n",
    "        temperature: Sampling temperature (higher = more random)\n",
    "        top_k: Top-k sampling parameter\n",
    "        top_p: Nucleus sampling parameter\n",
    "        num_return_sequences: Number of poems to generate\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Prepare input\n",
    "    if prompt:\n",
    "        input_text = f\"<|startoftext|>{prompt}\"\n",
    "    else:\n",
    "        input_text = \"<|startoftext|>\"\n",
    "    \n",
    "    input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
    "    \n",
    "    # Move to same device as model\n",
    "    device = next(model.parameters()).device\n",
    "    input_ids = input_ids.to(device)\n",
    "    \n",
    "    # Generate\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            input_ids,\n",
    "            max_length=max_length,\n",
    "            temperature=temperature,\n",
    "            top_k=top_k,\n",
    "            top_p=top_p,\n",
    "            num_return_sequences=num_return_sequences,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            eos_token_id=tokenizer.encode(\"<|endoftext|>\")[0]\n",
    "        )\n",
    "    \n",
    "    # Decode and clean up\n",
    "    poems = []\n",
    "    for sequence in output:\n",
    "        text = tokenizer.decode(sequence, skip_special_tokens=False)\n",
    "        # Remove special tokens for display\n",
    "        text = text.replace(\"<|startoftext|>\", \"\").replace(\"<|endoftext|>\", \"\")\n",
    "        text = text.strip()\n",
    "        poems.append(text)\n",
    "    \n",
    "    return poems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating poems with general topics...\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Poem 1:\n",
      "----------------------------------------------------------------------\n",
      "Sky lamp glints on us\n",
      "An amorous dusk it was\n",
      "We're merged with the stars and blown up by the sun's magic rays\n",
      "An amorous dawn it was\n",
      "We're merged with the galaxies and the dust clouds\n",
      "An amorous dusk it was\n",
      "We're merged with the stars and blown up by the light of the sun's full-moon\n",
      "An enchanting dawn it was\n",
      "We're merged with the planets and the moons\n",
      "An amorous dusk it was\n",
      "We're merged with the wind and the solar wind's magic\n",
      "An enchanting dawn it was\n",
      "We're merged with the stars and the planets\n",
      "An amorous dusk it was\n",
      "We're merged with the wind and the solar wind's magic\n",
      "An enchanting dawn it was\n",
      "We're merged with the stars and the planets\n",
      "An amorous dusk it was\n",
      "We're merged with the sun and the stars\n",
      "An enchanting dawn it was\n",
      "We're merged\n",
      "======================================================================\n",
      "\n",
      "Poem 2:\n",
      "----------------------------------------------------------------------\n",
      "On doggerel rime dedicated to you, catch my soul in narcotic romance,\n",
      "In rash I'll be disjointed; in a flash,  my spirit gratifies in hypnotic trance;\n",
      "For in my verse I find joy in my voice as you're my Hobson's choice!\n",
      "On the paper, I bare my soul to you as you are my vocalizing voice.\n",
      "Jingle bells, jingle bells, jingle in my ground with romantic sound!\n",
      "Thinking of you, I look at heavens and all I see is blue,\n",
      "Blue skies, white clouds, but not one trace of you oh baby, baby!\n",
      "I look deep into the dark night yet of no avail, I'm in love flu;\n",
      "So I surfed within my heart I saw a star with your smile, oh baby baby.\n",
      "Jingle bells, jingle bells, jingle in my ground with romantic sound!\n",
      "Let\n",
      "======================================================================\n",
      "\n",
      "Poem 3:\n",
      "----------------------------------------------------------------------\n",
      "Little romantic\n",
      "I am hasmukh* by name\n",
      "And can claim\n",
      "Full laughing stock\n",
      "With jokes\n",
      "\"Has\" means to smile\n",
      "\"Mukh\" means face\n",
      "It means smiling face\n",
      "No sadness in any case\n",
      "People may often comment\n",
      "You have right meaning in it\n",
      "But I know what hidden truth\n",
      "I feel it with each count of breathe\n",
      "I have moved\n",
      "And proved\n",
      "At each turn\n",
      "That I still learn\n",
      "I have yet to feel\n",
      "Pain and fill\n",
      "Their life with charm and happiness\n",
      "I want same smile on their face\n",
      "I am having little romantic nature\n",
      "And I see in it a good future\n",
      "I come across many friends\n",
      "With their leaning attitude in the end\n",
      "I come across many friends\n",
      "With their leaning attitude in the end\n",
      "I come across many friends\n",
      "With their leaning attitude in the end\n",
      "I come across many friends\n",
      "With their leaning attitude in the end\n",
      "I get\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Generate poems without specific prompts (general topics)\n",
    "print(\"Generating poems with general topics...\\n\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "general_poems = []\n",
    "for i in range(3):\n",
    "    poems = generate_poem(\n",
    "        model, \n",
    "        tokenizer, \n",
    "        prompt=\"\",  # No prompt - let model decide\n",
    "        max_length=200,\n",
    "        temperature=0.9,\n",
    "        num_return_sequences=1\n",
    "    )\n",
    "    general_poems.extend(poems)\n",
    "    print(f\"\\nPoem {i+1}:\")\n",
    "    print(\"-\"*70)\n",
    "    print(poems[0])\n",
    "    print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 and 3 has large amount of repeated sentence structures and have very few meaningful content. Moreover, they both end up exceeding the max length while outputing something tedious. The second poem is comparatively better and obviously it is about romance, the topic of which is the same as the inspiring set. Thus, we determine to modify the prompt for the generator to output specified romantic poems and see how it would perform. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Generate Romantic-Specific Poems\n",
    "\n",
    "Here we modify the prompts and keywords for romantic poem generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define romantic prompts and keywords\n",
    "romantic_prompts = [\n",
    "    \"Love\",\n",
    "    \"My heart\",\n",
    "    \"My dear\",\n",
    "    \"Sweet\",\n",
    "    \"Beloved\",\n",
    "    \"In your eyes\",\n",
    "    \"Your beauty\",\n",
    "    \"Romance\",\n",
    "    \"Passion\",\n",
    "    \"Tenderness\"\n",
    "]\n",
    "\n",
    "romantic_keywords = [\n",
    "    'love', 'heart', 'dear', 'sweet', 'beloved', 'kiss', 'embrace',\n",
    "    'passion', 'romance', 'tender', 'beauty', 'soul', 'desire', 'devotion',\n",
    "    'affection', 'cherish', 'adore', 'darling', 'sweetheart', 'angel'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_romantic_poem(text, keywords, threshold=2):\n",
    "    \"\"\"\n",
    "    Check if a poem contains romantic themes based on keyword presence.\n",
    "    \n",
    "    Args:\n",
    "        text: The poem text to check\n",
    "        keywords: List of romantic keywords\n",
    "        threshold: Minimum number of keywords required\n",
    "    \n",
    "    Returns:\n",
    "        Boolean indicating if poem is sufficiently romantic\n",
    "    \"\"\"\n",
    "    text_lower = text.lower()\n",
    "    count = sum(1 for keyword in keywords if keyword in text_lower)\n",
    "    return count >= threshold\n",
    "\n",
    "def generate_romantic_poem(model, tokenizer, max_attempts=10, **generation_kwargs):\n",
    "    \"\"\"\n",
    "    Generate a romantic poem by:\n",
    "    1. Using a romantic prompt\n",
    "    2. Filtering results to ensure romantic content\n",
    "    \"\"\"\n",
    "    for attempt in range(max_attempts):\n",
    "        # Select a random romantic prompt\n",
    "        prompt = random.choice(romantic_prompts)\n",
    "        \n",
    "        # Generate poem\n",
    "        poems = generate_poem(model, tokenizer, prompt=prompt, **generation_kwargs)\n",
    "        \n",
    "        # Check if poem is romantic enough\n",
    "        for poem in poems:\n",
    "            if is_romantic_poem(poem, romantic_keywords, threshold=2):\n",
    "                return poem, prompt\n",
    "    \n",
    "    # If we couldn't generate a sufficiently romantic poem, return the last one\n",
    "    return poems[0], prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating romantic-specific poems...\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Romantic Poem 1 (started with: 'My heart'):\n",
      "----------------------------------------------------------------------\n",
      "My heart overflows\n",
      "With sweet romantic feelings\n",
      "When I think of you,\n",
      "My eyes are missing you,\n",
      "I wish that your family agrees to marry me\n",
      "In a romantic ceremony,\n",
      "But in vain! My heart is over-strained,\n",
      "I am in love,\n",
      "And I want nothing more than to be with you!\n",
      "When I was young,\n",
      "My whole world was consumed by your splendour,\n",
      "I imagined you,\n",
      "The wise ones,\n",
      "The beautiful ones,\n",
      "Who had attained the loftiest heights,\n",
      "Who had enjoyed the finest soil,\n",
      "Who had lived in the deepest parts of the earth!\n",
      "I fancied you still live,\n",
      "The bloated corpses of your people,\n",
      "Who starved to death on the Egyptian highroad!\n",
      "When I look into your eyes,\n",
      "The deep sadness in your soul,\n",
      "I wish that your descendants will never forget me!\n",
      "When I look into your dreams,\n",
      "======================================================================\n",
      "\n",
      "Romantic Poem 2 (started with: 'Love'):\n",
      "----------------------------------------------------------------------\n",
      "Love the heart of all romance,\n",
      "Must be of the sacred, laughing brook,\n",
      "It must be of the eternal dance.\n",
      "It must of her sable, dark look,\n",
      "And of mine returning purity for purity.\n",
      "Romance is love and love is romance.\n",
      "It is of the azure, gleaming stream,\n",
      "It is glory beyond all ecstasy.\n",
      "It transcends all pines, all wines, all loftiness,\n",
      "It is not a fabled, windy dream.\n",
      "And when one is immersed in its ineffable caress,\n",
      "When diamonds are donning her wedding dress,\n",
      "The sun and the stars rise merely to greet\n",
      "Yet another height where lips do meet.\n",
      "Then alone they are alone.\n",
      "When lips do meet, and lips do meet,\n",
      "When lips do meet and lips do meet,\n",
      "And lips do meet and lips do meet,\n",
      "And lips do meet and lips meet\n",
      "======================================================================\n",
      "\n",
      "Romantic Poem 3 (started with: 'Beloved'):\n",
      "----------------------------------------------------------------------\n",
      "Beloved and lost,\n",
      "lost and never found,\n",
      "lost and never found.\n",
      "Babe,\n",
      "your love is my whole,\n",
      "my whole,\n",
      "even if only for a moment.\n",
      "Debate,\n",
      "latch,\n",
      "date,\n",
      "love,\n",
      "offer…\n",
      "but never forget…\n",
      "your love is my whole,\n",
      "even if only for a moment.\n",
      "Romantic poem,\n",
      "biography,\n",
      "lecture,\n",
      "life-pregnant,\n",
      "wounded,\n",
      "smell,\n",
      "smell…\n",
      "ever in my soul.\n",
      "As your love, my whole,\n",
      "beyond your dreams…\n",
      "and my dreams.\n",
      "As your love, my whole,\n",
      "beyond your ceaseless stares.\n",
      "Love you, my whole,\n",
      "’Kiss, kiss me…\n",
      "’mourn I for you…\n",
      "’never forget…\n",
      "my love….\n",
      "and yours.\n",
      "Babe\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Generate romantic-specific poems\n",
    "print(\"Generating romantic-specific poems...\\n\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "romantic_poems = []\n",
    "for i in range(3):\n",
    "    poem, prompt = generate_romantic_poem(\n",
    "        model,\n",
    "        tokenizer,\n",
    "        max_length=200,\n",
    "        temperature=0.85,  # Slightly lower temperature for more coherence\n",
    "        top_k=50,\n",
    "        top_p=0.95,\n",
    "        num_return_sequences=1\n",
    "    )\n",
    "    romantic_poems.append(poem)\n",
    "    \n",
    "    print(f\"\\nRomantic Poem {i+1} (started with: '{prompt}'):\")\n",
    "    print(\"-\"*70)\n",
    "    print(poem)\n",
    "    print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Romantic poems are much more meaningful with romance-related contexts, since they have a better fine-grained weights on romantic corpus and the romantic poem structure. The 1st poem still has length-exceeded issue but the overall quality is still better than general poems, even compared with the best general one(the second from general poems). It even starts to rhyme with some words, for example in Romantic Poem 1 `marry me` and `ceremony`, `forget me` and `dreams`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 3 general poems and 3 romantic poems to 'generated_poems/' folder\n"
     ]
    }
   ],
   "source": [
    "# Save all generated poems to files\n",
    "import os\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(\"generated_poems\", exist_ok=True)\n",
    "\n",
    "# Save general poems\n",
    "for i, poem in enumerate(general_poems):\n",
    "    with open(f\"generated_poems/general_poem_{i+1}.txt\", 'w', encoding='utf-8') as f:\n",
    "        f.write(poem)\n",
    "\n",
    "# Save romantic poems\n",
    "for i, poem in enumerate(romantic_poems):\n",
    "    with open(f\"generated_poems/romantic_poem_{i+1}.txt\", 'w', encoding='utf-8') as f:\n",
    "        f.write(poem)\n",
    "\n",
    "print(f\"Saved {len(general_poems)} general poems and {len(romantic_poems)} romantic poems to 'generated_poems/' folder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creativity Evaluation\n",
    "\n",
    "This section evaluates the generated poems across four key dimensions:\n",
    "1. Novelty - How different are they from the training data?\n",
    "2. Coherence - Do the poems make logical and linguistic sense?\n",
    "3. Emotional Impact - Do they evoke romantic feelings?\n",
    "4. Technical Quality - Rhyme, rhythm, and imagery\n",
    "\n",
    "We evaluate with both automatic method and our own thoughts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# METRIC 1: NOVELTY - Comparing with Inspiring Set\n",
    "import difflib\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "def compute_n_grams(text, n=3):\n",
    "    \"\"\"Extract n-grams from text for similarity comparison.\"\"\"\n",
    "    words = text.lower().split()\n",
    "    return [' '.join(words[i:i+n]) for i in range(len(words)-n+1)]\n",
    "\n",
    "def compute_novelty_score(generated_poem, training_poems, n=3):\n",
    "    \"\"\"\n",
    "    Calculate novelty by measuring n-gram overlap with training data.\n",
    "    Returns a score between 0 and 1, where 1 means completely novel.\n",
    "    \"\"\"\n",
    "    gen_ngrams = set(compute_n_grams(generated_poem, n))\n",
    "    \n",
    "    if not gen_ngrams:\n",
    "        return 0.0\n",
    "    \n",
    "    # Check overlap with each training poem\n",
    "    max_overlap = 0\n",
    "    for train_poem in training_poems:\n",
    "        train_ngrams = set(compute_n_grams(train_poem, n))\n",
    "        if train_ngrams:\n",
    "            overlap = len(gen_ngrams.intersection(train_ngrams)) / len(gen_ngrams)\n",
    "            max_overlap = max(max_overlap, overlap)\n",
    "    \n",
    "    # Novelty is inverse of maximum overlap\n",
    "    novelty = 1 - max_overlap\n",
    "    return novelty\n",
    "\n",
    "def compute_exact_match_ratio(generated_poem, training_poems):\n",
    "    \"\"\"\n",
    "    Check if any substantial phrase from generated poem appears verbatim in training data.\n",
    "    \"\"\"\n",
    "    gen_text = generated_poem.lower()\n",
    "    \n",
    "    for train_poem in training_poems:\n",
    "        train_text = train_poem.lower()\n",
    "        # Use SequenceMatcher to find longest common substring\n",
    "        matcher = difflib.SequenceMatcher(None, gen_text, train_text)\n",
    "        match = matcher.find_longest_match(0, len(gen_text), 0, len(train_text))\n",
    "        \n",
    "        if match.size > 30:  # If match is longer than 30 characters\n",
    "            return match.size, train_text[match.b:match.b+match.size]\n",
    "    \n",
    "    return 0, \"\"\n",
    "\n",
    "def evaluate_novelty(generated_poems, training_poems):\n",
    "    \"\"\"\n",
    "    Comprehensive novelty evaluation.\n",
    "    \"\"\"\n",
    "    print(\"METRIC 1: NOVELTY EVALUATION\")\n",
    "    print(f\"\\nEvaluating {len(generated_poems)} generated poems against {len(training_poems)} training poems\\n\")\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for i, poem in enumerate(generated_poems):\n",
    "        print(f\"\\n--- Poem {i+1} ---\")\n",
    "        \n",
    "        # Compute novelty scores at different n-gram levels\n",
    "        trigram_novelty = compute_novelty_score(poem, training_poems, n=3)\n",
    "        bigram_novelty = compute_novelty_score(poem, training_poems, n=2)\n",
    "        \n",
    "        # Check for exact matches\n",
    "        match_length, match_text = compute_exact_match_ratio(poem, training_poems)\n",
    "        \n",
    "        results.append({\n",
    "            'poem_id': i+1,\n",
    "            'trigram_novelty': trigram_novelty,\n",
    "            'bigram_novelty': bigram_novelty,\n",
    "            'exact_match_length': match_length\n",
    "        })\n",
    "        \n",
    "        print(f\"Trigram Novelty Score: {trigram_novelty:.3f} (1.0 = completely novel)\")\n",
    "        print(f\"Bigram Novelty Score: {bigram_novelty:.3f}\")\n",
    "        \n",
    "        if match_length > 30:\n",
    "            print(f\"  Found exact match ({match_length} chars): '{match_text[:50]}...'\")\n",
    "        else:\n",
    "            print(\" No substantial exact matches found\")\n",
    "        \n",
    "        # Overall assessment\n",
    "        avg_novelty = (trigram_novelty + bigram_novelty) / 2\n",
    "        if avg_novelty > 0.8:\n",
    "            print(f\"Assessment: HIGH NOVELTY \")\n",
    "        elif avg_novelty > 0.5:\n",
    "            print(f\"Assessment: MODERATE NOVELTY\")\n",
    "        else:\n",
    "            print(f\"Assessment: LOW NOVELTY \")\n",
    "    \n",
    "    # Summary statistics\n",
    "    avg_trigram = sum(r['trigram_novelty'] for r in results) / len(results)\n",
    "    avg_bigram = sum(r['bigram_novelty'] for r in results) / len(results)\n",
    "\n",
    "    print(\"NOVELTY SUMMARY:\")\n",
    "    print(f\"Average Trigram Novelty: {avg_trigram:.3f}\")\n",
    "    print(f\"Average Bigram Novelty: {avg_bigram:.3f}\")\n",
    "    print(f\"Overall Novelty Score: {(avg_trigram + avg_bigram) / 2:.3f}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# METRIC 2: COHERENCE - Linguistic and Logical Sense\n",
    "\n",
    "def evaluate_coherence_metrics(poem):\n",
    "    \"\"\"\n",
    "    Evaluate various aspects of coherence.\n",
    "    \"\"\"\n",
    "    lines = [line.strip() for line in poem.split('\\n') if line.strip()]\n",
    "    \n",
    "    metrics = {\n",
    "        'num_lines': len(lines),\n",
    "        'avg_line_length': sum(len(line.split()) for line in lines) / len(lines) if lines else 0,\n",
    "        'has_punctuation': bool(re.search(r'[.!?,;:]', poem)),\n",
    "        'repeated_words': 0,\n",
    "        'sentence_fragments': 0,\n",
    "    }\n",
    "    \n",
    "    # Check for excessive word repetition\n",
    "    words = poem.lower().split()\n",
    "    word_counts = Counter(words)\n",
    "    metrics['repeated_words'] = sum(1 for count in word_counts.values() if count > 3)\n",
    "    \n",
    "    # Check for very short lines (potential fragments)\n",
    "    metrics['sentence_fragments'] = sum(1 for line in lines if len(line.split()) < 3)\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def coherence_human_readable_assessment(poem_text):\n",
    "    \"\"\"\n",
    "    This function returns a structured assessment for human evaluation.\n",
    "    The actual coherence judgment should be done by reading the poem.\n",
    "    \"\"\"\n",
    "    metrics = evaluate_coherence_metrics(poem_text)\n",
    "    \n",
    "    assessment = {\n",
    "        'structure': 'Good' if 5 <= metrics['num_lines'] <= 30 else 'Unusual',\n",
    "        'line_length': 'Consistent' if 3 <= metrics['avg_line_length'] <= 12 else 'Variable',\n",
    "        'punctuation': 'Present' if metrics['has_punctuation'] else 'Missing',\n",
    "        'repetition': 'Excessive' if metrics['repeated_words'] > 2 else 'Appropriate',\n",
    "        'fragments': 'Many' if metrics['sentence_fragments'] > metrics['num_lines'] * 0.3 else 'Few'\n",
    "    }\n",
    "    \n",
    "    return assessment, metrics\n",
    "\n",
    "def evaluate_coherence(poems):\n",
    "    \"\"\"\n",
    "    Evaluate coherence with automatic metrics and structure for human judgment.\n",
    "    \"\"\"\n",
    "    print(\"\\nMETRIC 2: COHERENCE EVALUATION\")\n",
    "    print(\"\\nNote: Coherence requires human judgment. These are supporting metrics.\\n\")\n",
    "    \n",
    "    for i, poem in enumerate(poems):\n",
    "        print(f\"\\n--- Poem {i+1} ---\")\n",
    "        print(\"Poem Text:\")\n",
    "        print(poem[:300] + \"...\" if len(poem) > 300 else poem)\n",
    "        \n",
    "        assessment, metrics = coherence_human_readable_assessment(poem)\n",
    "        \n",
    "        print(f\"\\nAutomatic Metrics:\")\n",
    "        print(f\"  Lines: {metrics['num_lines']}\")\n",
    "        print(f\"  Avg words/line: {metrics['avg_line_length']:.1f}\")\n",
    "        print(f\"  Punctuation: {assessment['punctuation']}\")\n",
    "        print(f\"  Word repetition: {assessment['repetition']}\")\n",
    "        print(f\"  Sentence fragments: {assessment['fragments']}\")\n",
    "\n",
    "\n",
    "# METRIC 3: EMOTIONAL IMPACT - Romantic Feelings\n",
    "\n",
    "def analyze_emotional_vocabulary(poem):\n",
    "    \"\"\"\n",
    "    Analyze the presence of emotion-evoking vocabulary.\n",
    "    \"\"\"\n",
    "    # Emotional vocabulary categories\n",
    "    emotion_words = {\n",
    "        'love': ['love', 'adore', 'cherish', 'devotion', 'affection', 'passion'],\n",
    "        'longing': ['yearn', 'desire', 'miss', 'wish', 'long', 'crave', 'ache'],\n",
    "        'beauty': ['beautiful', 'lovely', 'gorgeous', 'fair', 'radiant', 'exquisite'],\n",
    "        'intimacy': ['kiss', 'embrace', 'touch', 'caress', 'tender', 'gentle'],\n",
    "        'joy': ['delight', 'bliss', 'happiness', 'joy', 'ecstasy', 'rapture'],\n",
    "        'sadness': ['sorrow', 'melancholy', 'tears', 'pain', 'heartbreak', 'mourn']\n",
    "    }\n",
    "    \n",
    "    poem_lower = poem.lower()\n",
    "    found_emotions = {}\n",
    "    \n",
    "    for category, words in emotion_words.items():\n",
    "        found = [word for word in words if word in poem_lower]\n",
    "        if found:\n",
    "            found_emotions[category] = found\n",
    "    \n",
    "    return found_emotions\n",
    "\n",
    "def evaluate_emotional_impact(poems):\n",
    "    \"\"\"\n",
    "    Evaluate the emotional impact and romantic quality of poems.\n",
    "    \"\"\"\n",
    "    print(\"\\nMETRIC 3: EMOTIONAL IMPACT EVALUATION\")\n",
    "    print(\"\\nAnalyzing romantic and emotional vocabulary...\\n\")\n",
    "    \n",
    "    for i, poem in enumerate(poems):\n",
    "        print(f\"\\n--- Poem {i+1} ---\")\n",
    "        print(\"Poem Text:\")\n",
    "        print(poem[:300] + \"...\" if len(poem) > 300 else poem)\n",
    "        \n",
    "        emotions = analyze_emotional_vocabulary(poem)\n",
    "        \n",
    "        print(f\"\\nEmotional Vocabulary Analysis:\")\n",
    "        if emotions:\n",
    "            for category, words in emotions.items():\n",
    "                print(f\"  {category.upper()}: {', '.join(words)}\")\n",
    "            print(f\"\\n  Total emotional categories: {len(emotions)}/6\")\n",
    "            \n",
    "            # Score\n",
    "            emotion_score = len(emotions) / 6\n",
    "            if emotion_score > 0.5:\n",
    "                print(f\"  Emotional richness: HIGH \")\n",
    "            elif emotion_score > 0.3:\n",
    "                print(f\"  Emotional richness: MODERATE\")\n",
    "            else:\n",
    "                print(f\"  Emotional richness: LOW\")\n",
    "        else:\n",
    "            print(\"  No strong emotional vocabulary detected \")\n",
    "    \n",
    "\n",
    "# METRIC 4: TECHNICAL QUALITY - Rhyme, Rhythm, Imagery\n",
    "\n",
    "def detect_rhyme_scheme(poem):\n",
    "    \"\"\"\n",
    "    Simple rhyme detection based on line endings.\n",
    "    \"\"\"\n",
    "    lines = [line.strip() for line in poem.split('\\n') if line.strip()]\n",
    "    \n",
    "    if len(lines) < 2:\n",
    "        return None, []\n",
    "    \n",
    "    # Get last words\n",
    "    last_words = []\n",
    "    for line in lines:\n",
    "        words = re.findall(r'\\b\\w+\\b', line.lower())\n",
    "        if words:\n",
    "            last_words.append(words[-1])\n",
    "    \n",
    "    # Check for rhyming pairs (simple phonetic similarity)\n",
    "    rhyme_pairs = []\n",
    "    for i in range(len(last_words) - 1):\n",
    "        for j in range(i + 1, len(last_words)):\n",
    "            if last_words[i][-2:] == last_words[j][-2:] and len(last_words[i]) > 2:\n",
    "                rhyme_pairs.append((i, j, last_words[i], last_words[j]))\n",
    "    \n",
    "    return last_words, rhyme_pairs\n",
    "\n",
    "def analyze_rhythm(poem):\n",
    "    \"\"\"\n",
    "    Basic rhythm analysis - syllable patterns.\n",
    "    \"\"\"\n",
    "    lines = [line.strip() for line in poem.split('\\n') if line.strip()]\n",
    "    \n",
    "    # Approximate syllable count (very rough)\n",
    "    def count_syllables_rough(line):\n",
    "        vowels = 'aeiouAEIOU'\n",
    "        syllables = 0\n",
    "        previous_was_vowel = False\n",
    "        for char in line:\n",
    "            is_vowel = char in vowels\n",
    "            if is_vowel and not previous_was_vowel:\n",
    "                syllables += 1\n",
    "            previous_was_vowel = is_vowel\n",
    "        return max(1, syllables)\n",
    "    \n",
    "    syllable_counts = [count_syllables_rough(line) for line in lines]\n",
    "    \n",
    "    if not syllable_counts:\n",
    "        return {'pattern': 'None', 'consistency': 0}\n",
    "    \n",
    "    # Check consistency\n",
    "    avg_syllables = sum(syllable_counts) / len(syllable_counts)\n",
    "    variance = sum((c - avg_syllables) ** 2 for c in syllable_counts) / len(syllable_counts)\n",
    "    consistency = 1 / (1 + variance)  # 1 = perfect consistency\n",
    "    \n",
    "    return {\n",
    "        'avg_syllables': avg_syllables,\n",
    "        'consistency': consistency,\n",
    "        'pattern': syllable_counts[:min(5, len(syllable_counts))]\n",
    "    }\n",
    "\n",
    "def analyze_imagery(poem):\n",
    "    \"\"\"\n",
    "    Detect imagery and figurative language.\n",
    "    \"\"\"\n",
    "    imagery_words = {\n",
    "        'visual': ['see', 'look', 'gaze', 'bright', 'dark', 'shimmer', 'glow', 'shadow'],\n",
    "        'tactile': ['touch', 'soft', 'warm', 'cold', 'smooth', 'rough'],\n",
    "        'auditory': ['hear', 'sound', 'whisper', 'song', 'music', 'voice'],\n",
    "        'nature': ['rose', 'flower', 'moon', 'sun', 'star', 'sky', 'ocean', 'wind']\n",
    "    }\n",
    "    \n",
    "    poem_lower = poem.lower()\n",
    "    found_imagery = {}\n",
    "    \n",
    "    for category, words in imagery_words.items():\n",
    "        found = [word for word in words if word in poem_lower]\n",
    "        if found:\n",
    "            found_imagery[category] = found\n",
    "    \n",
    "    return found_imagery\n",
    "\n",
    "def evaluate_technical_quality(poems):\n",
    "    \"\"\"\n",
    "    Evaluate technical aspects: rhyme, rhythm, and imagery.\n",
    "    \"\"\"\n",
    "    print(\"\\nMETRIC 4: TECHNICAL QUALITY EVALUATION\")\n",
    "    print(\"\\nAnalyzing rhyme scheme, rhythm patterns, and imagery...\\n\")\n",
    "    \n",
    "    for i, poem in enumerate(poems):\n",
    "        print(f\"\\n--- Poem {i+1} ---\")\n",
    "        print(\"Poem Text:\")\n",
    "        print(poem[:300] + \"...\" if len(poem) > 300 else poem)\n",
    "        \n",
    "        # Rhyme analysis\n",
    "        last_words, rhyme_pairs = detect_rhyme_scheme(poem)\n",
    "        print(f\"\\n RHYME ANALYSIS:\")\n",
    "        if rhyme_pairs:\n",
    "            print(f\"  Rhyming pairs found: {len(rhyme_pairs)}\")\n",
    "            for i1, i2, w1, w2 in rhyme_pairs[:3]:  # Show first 3\n",
    "                print(f\"    Line {i1+1} ('{w1}') rhymes with Line {i2+1} ('{w2}')\")\n",
    "            print(f\"  Rhyme presence: YES \")\n",
    "        else:\n",
    "            print(f\"  Rhyme presence: NONE or MINIMAL\")\n",
    "        \n",
    "        # Rhythm analysis\n",
    "        rhythm = analyze_rhythm(poem)\n",
    "        print(f\"\\n RHYTHM ANALYSIS:\")\n",
    "        print(f\"  Avg syllables/line: {rhythm['avg_syllables']:.1f}\")\n",
    "        print(f\"  Rhythm consistency: {rhythm['consistency']:.2f} (1.0 = perfect)\")\n",
    "        print(f\"  Pattern sample: {rhythm['pattern']}\")\n",
    "        \n",
    "        # Imagery analysis\n",
    "        imagery = analyze_imagery(poem)\n",
    "        print(f\"\\n IMAGERY ANALYSIS:\")\n",
    "        if imagery:\n",
    "            for category, words in imagery.items():\n",
    "                print(f\"  {category.upper()}: {', '.join(words)}\")\n",
    "            print(f\"  Imagery richness: {len(imagery)}/4 categories\")\n",
    "        else:\n",
    "            print(f\"  Imagery: MINIMAL \")\n",
    "        \n",
    "        # Overall technical assessment\n",
    "        print(f\"\\n TECHNICAL ASSESSMENT:\")\n",
    "        tech_score = 0\n",
    "        if rhyme_pairs:\n",
    "            tech_score += 1\n",
    "            print(f\"   Has rhyme scheme\")\n",
    "        if rhythm['consistency'] > 0.7:\n",
    "            tech_score += 1\n",
    "            print(f\"   Consistent rhythm\")\n",
    "        if len(imagery) >= 2:\n",
    "            tech_score += 1\n",
    "            print(f\"   Good imagery\")\n",
    "        \n",
    "        if tech_score >= 2:\n",
    "            print(f\"\\n  Overall: GOOD TECHNICAL QUALITY \")\n",
    "        elif tech_score == 1:\n",
    "            print(f\"\\n  Overall: MODERATE TECHNICAL QUALITY\")\n",
    "        else:\n",
    "            print(f\"\\n  Overall: LIMITED TECHNICAL QUALITY\")\n",
    "    \n",
    "\n",
    "\n",
    "def run_comprehensive_evaluation(generated_poems, training_poems):\n",
    "    \"\"\"\n",
    "    Run all four evaluation metrics.\n",
    "    \"\"\"\n",
    "    print(\"\\n\")\n",
    "    print( \"COMPREHENSIVE CREATIVITY EVALUATION\\n\")\n",
    "    \n",
    "    # Metric 1: Novelty\n",
    "    novelty_results = evaluate_novelty(generated_poems, training_poems)\n",
    "    \n",
    "    # Metric 2: Coherence\n",
    "    evaluate_coherence(generated_poems)\n",
    "    \n",
    "    # Metric 3: Emotional Impact\n",
    "    evaluate_emotional_impact(generated_poems)\n",
    "    \n",
    "    # Metric 4: Technical Quality\n",
    "    evaluate_technical_quality(generated_poems)\n",
    "\n",
    "    print(\"EVALUATION COMPLETE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "COMPREHENSIVE CREATIVITY EVALUATION\n",
      "\n",
      "METRIC 1: NOVELTY EVALUATION\n",
      "\n",
      "Evaluating 3 generated poems against 1 training poems\n",
      "\n",
      "\n",
      "--- Poem 1 ---\n",
      "Trigram Novelty Score: 1.000 (1.0 = completely novel)\n",
      "Bigram Novelty Score: 0.961\n",
      " No substantial exact matches found\n",
      "Assessment: HIGH NOVELTY \n",
      "\n",
      "--- Poem 2 ---\n",
      "Trigram Novelty Score: 1.000 (1.0 = completely novel)\n",
      "Bigram Novelty Score: 1.000\n",
      " No substantial exact matches found\n",
      "Assessment: HIGH NOVELTY \n",
      "\n",
      "--- Poem 3 ---\n",
      "Trigram Novelty Score: 1.000 (1.0 = completely novel)\n",
      "Bigram Novelty Score: 1.000\n",
      " No substantial exact matches found\n",
      "Assessment: HIGH NOVELTY \n",
      "NOVELTY SUMMARY:\n",
      "Average Trigram Novelty: 1.000\n",
      "Average Bigram Novelty: 0.987\n",
      "Overall Novelty Score: 0.993\n",
      "\n",
      "METRIC 2: COHERENCE EVALUATION\n",
      "\n",
      "Note: Coherence requires human judgment. These are supporting metrics.\n",
      "\n",
      "\n",
      "--- Poem 1 ---\n",
      "Poem Text:\n",
      "My heart overflows\n",
      "With sweet romantic feelings\n",
      "When I think of you,\n",
      "My eyes are missing you,\n",
      "I wish that your family agrees to marry me\n",
      "In a romantic ceremony,\n",
      "But in vain! My heart is over-strained,\n",
      "I am in love,\n",
      "And I want nothing more than to be with you!\n",
      "When I was young,\n",
      "My whole world was con...\n",
      "\n",
      "Automatic Metrics:\n",
      "  Lines: 24\n",
      "  Avg words/line: 5.8\n",
      "  Punctuation: Present\n",
      "  Word repetition: Excessive\n",
      "  Sentence fragments: Few\n",
      "\n",
      "--- Poem 2 ---\n",
      "Poem Text:\n",
      "Love the heart of all romance,\n",
      "Must be of the sacred, laughing brook,\n",
      "It must be of the eternal dance.\n",
      "It must of her sable, dark look,\n",
      "And of mine returning purity for purity.\n",
      "Romance is love and love is romance.\n",
      "It is of the azure, gleaming stream,\n",
      "It is glory beyond all ecstasy.\n",
      "It transcends all...\n",
      "\n",
      "Automatic Metrics:\n",
      "  Lines: 19\n",
      "  Avg words/line: 7.2\n",
      "  Punctuation: Present\n",
      "  Word repetition: Excessive\n",
      "  Sentence fragments: Few\n",
      "\n",
      "--- Poem 3 ---\n",
      "Poem Text:\n",
      "Beloved and lost,\n",
      "lost and never found,\n",
      "lost and never found.\n",
      "Babe,\n",
      "your love is my whole,\n",
      "my whole,\n",
      "even if only for a moment.\n",
      "Debate,\n",
      "latch,\n",
      "date,\n",
      "love,\n",
      "offer…\n",
      "but never forget…\n",
      "your love is my whole,\n",
      "even if only for a moment.\n",
      "Romantic poem,\n",
      "biography,\n",
      "lecture,\n",
      "life-pregnant,\n",
      "wounded,\n",
      "smell,\n",
      "smel...\n",
      "\n",
      "Automatic Metrics:\n",
      "  Lines: 35\n",
      "  Avg words/line: 2.7\n",
      "  Punctuation: Present\n",
      "  Word repetition: Excessive\n",
      "  Sentence fragments: Many\n",
      "\n",
      "METRIC 3: EMOTIONAL IMPACT EVALUATION\n",
      "\n",
      "Analyzing romantic and emotional vocabulary...\n",
      "\n",
      "\n",
      "--- Poem 1 ---\n",
      "Poem Text:\n",
      "My heart overflows\n",
      "With sweet romantic feelings\n",
      "When I think of you,\n",
      "My eyes are missing you,\n",
      "I wish that your family agrees to marry me\n",
      "In a romantic ceremony,\n",
      "But in vain! My heart is over-strained,\n",
      "I am in love,\n",
      "And I want nothing more than to be with you!\n",
      "When I was young,\n",
      "My whole world was con...\n",
      "\n",
      "Emotional Vocabulary Analysis:\n",
      "  LOVE: love\n",
      "  LONGING: miss, wish\n",
      "  BEAUTY: beautiful\n",
      "  JOY: joy\n",
      "\n",
      "  Total emotional categories: 4/6\n",
      "  Emotional richness: HIGH \n",
      "\n",
      "--- Poem 2 ---\n",
      "Poem Text:\n",
      "Love the heart of all romance,\n",
      "Must be of the sacred, laughing brook,\n",
      "It must be of the eternal dance.\n",
      "It must of her sable, dark look,\n",
      "And of mine returning purity for purity.\n",
      "Romance is love and love is romance.\n",
      "It is of the azure, gleaming stream,\n",
      "It is glory beyond all ecstasy.\n",
      "It transcends all...\n",
      "\n",
      "Emotional Vocabulary Analysis:\n",
      "  LOVE: love\n",
      "  INTIMACY: caress\n",
      "  JOY: ecstasy\n",
      "\n",
      "  Total emotional categories: 3/6\n",
      "  Emotional richness: MODERATE\n",
      "\n",
      "--- Poem 3 ---\n",
      "Poem Text:\n",
      "Beloved and lost,\n",
      "lost and never found,\n",
      "lost and never found.\n",
      "Babe,\n",
      "your love is my whole,\n",
      "my whole,\n",
      "even if only for a moment.\n",
      "Debate,\n",
      "latch,\n",
      "date,\n",
      "love,\n",
      "offer…\n",
      "but never forget…\n",
      "your love is my whole,\n",
      "even if only for a moment.\n",
      "Romantic poem,\n",
      "biography,\n",
      "lecture,\n",
      "life-pregnant,\n",
      "wounded,\n",
      "smell,\n",
      "smel...\n",
      "\n",
      "Emotional Vocabulary Analysis:\n",
      "  LOVE: love\n",
      "  INTIMACY: kiss\n",
      "  SADNESS: mourn\n",
      "\n",
      "  Total emotional categories: 3/6\n",
      "  Emotional richness: MODERATE\n",
      "\n",
      "METRIC 4: TECHNICAL QUALITY EVALUATION\n",
      "\n",
      "Analyzing rhyme scheme, rhythm patterns, and imagery...\n",
      "\n",
      "\n",
      "--- Poem 1 ---\n",
      "Poem Text:\n",
      "My heart overflows\n",
      "With sweet romantic feelings\n",
      "When I think of you,\n",
      "My eyes are missing you,\n",
      "I wish that your family agrees to marry me\n",
      "In a romantic ceremony,\n",
      "But in vain! My heart is over-strained,\n",
      "I am in love,\n",
      "And I want nothing more than to be with you!\n",
      "When I was young,\n",
      "My whole world was con...\n",
      "\n",
      " RHYME ANALYSIS:\n",
      "  Rhyming pairs found: 10\n",
      "    Line 3 ('you') rhymes with Line 4 ('you')\n",
      "    Line 3 ('you') rhymes with Line 9 ('you')\n",
      "    Line 3 ('you') rhymes with Line 12 ('you')\n",
      "  Rhyme presence: YES \n",
      "\n",
      " RHYTHM ANALYSIS:\n",
      "  Avg syllables/line: 8.0\n",
      "  Rhythm consistency: 0.13 (1.0 = perfect)\n",
      "  Pattern sample: [4, 7, 5, 7, 11]\n",
      "\n",
      " IMAGERY ANALYSIS:\n",
      "  VISUAL: look\n",
      "  AUDITORY: hear\n",
      "  NATURE: star\n",
      "  Imagery richness: 3/4 categories\n",
      "\n",
      " TECHNICAL ASSESSMENT:\n",
      "   Has rhyme scheme\n",
      "   Good imagery\n",
      "\n",
      "  Overall: GOOD TECHNICAL QUALITY \n",
      "\n",
      "--- Poem 2 ---\n",
      "Poem Text:\n",
      "Love the heart of all romance,\n",
      "Must be of the sacred, laughing brook,\n",
      "It must be of the eternal dance.\n",
      "It must of her sable, dark look,\n",
      "And of mine returning purity for purity.\n",
      "Romance is love and love is romance.\n",
      "It is of the azure, gleaming stream,\n",
      "It is glory beyond all ecstasy.\n",
      "It transcends all...\n",
      "\n",
      " RHYME ANALYSIS:\n",
      "  Rhyming pairs found: 23\n",
      "    Line 1 ('romance') rhymes with Line 3 ('dance')\n",
      "    Line 1 ('romance') rhymes with Line 6 ('romance')\n",
      "    Line 2 ('brook') rhymes with Line 4 ('look')\n",
      "  Rhyme presence: YES \n",
      "\n",
      " RHYTHM ANALYSIS:\n",
      "  Avg syllables/line: 9.9\n",
      "  Rhythm consistency: 0.17 (1.0 = perfect)\n",
      "  Pattern sample: [9, 9, 10, 8, 12]\n",
      "\n",
      " IMAGERY ANALYSIS:\n",
      "  VISUAL: look, dark\n",
      "  AUDITORY: hear\n",
      "  NATURE: sun, star, wind\n",
      "  Imagery richness: 3/4 categories\n",
      "\n",
      " TECHNICAL ASSESSMENT:\n",
      "   Has rhyme scheme\n",
      "   Good imagery\n",
      "\n",
      "  Overall: GOOD TECHNICAL QUALITY \n",
      "\n",
      "--- Poem 3 ---\n",
      "Poem Text:\n",
      "Beloved and lost,\n",
      "lost and never found,\n",
      "lost and never found.\n",
      "Babe,\n",
      "your love is my whole,\n",
      "my whole,\n",
      "even if only for a moment.\n",
      "Debate,\n",
      "latch,\n",
      "date,\n",
      "love,\n",
      "offer…\n",
      "but never forget…\n",
      "your love is my whole,\n",
      "even if only for a moment.\n",
      "Romantic poem,\n",
      "biography,\n",
      "lecture,\n",
      "life-pregnant,\n",
      "wounded,\n",
      "smell,\n",
      "smel...\n",
      "\n",
      " RHYME ANALYSIS:\n",
      "  Rhyming pairs found: 25\n",
      "    Line 2 ('found') rhymes with Line 3 ('found')\n",
      "    Line 4 ('babe') rhymes with Line 35 ('babe')\n",
      "    Line 5 ('whole') rhymes with Line 6 ('whole')\n",
      "  Rhyme presence: YES \n",
      "\n",
      " RHYTHM ANALYSIS:\n",
      "  Avg syllables/line: 3.7\n",
      "  Rhythm consistency: 0.20 (1.0 = perfect)\n",
      "  Pattern sample: [5, 5, 5, 2, 6]\n",
      "\n",
      " IMAGERY ANALYSIS:\n",
      "  NATURE: star\n",
      "  Imagery richness: 1/4 categories\n",
      "\n",
      " TECHNICAL ASSESSMENT:\n",
      "   Has rhyme scheme\n",
      "\n",
      "  Overall: MODERATE TECHNICAL QUALITY\n",
      "EVALUATION COMPLETE\n"
     ]
    }
   ],
   "source": [
    "run_comprehensive_evaluation(romantic_poems, poems)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first generated poem presents a complex case of creative failure despite high novelty. The poem begins promisingly with conventional romantic imagery and sentiment, opening with \"My heart overflows / With sweet romantic feelings / When I think of you.\" This initial section establishes clear romantic intent, expressing desire for marriage and devotion to the beloved. However, the poem undergoes a severe and inexplicable tonal shift approximately halfway through, introducing disturbing historical imagery including \"bloated corpses\" and references to starvation on an \"Egyptian highroad.\"\n",
    "\n",
    "From a coherence perspective, this poem fails fundamentally. While it maintains grammatical correctness and a consistent first-person perspective, it does not follow a logical flow. The transition from contemporary romantic confession to morbid historical commentary occurs without explanation or connection. The ideas are not coherently linked; the romantic plea to marry and the graphic death imagery exist as two incompatible fragments forced into proximity. The poem reads as though two entirely different compositions were spliced together mid-generation, suggesting a failure in the model's ability to maintain thematic consistency over longer sequences.\n",
    "\n",
    "The emotional impact of this poem is similarly problematic. Despite the automated metrics detecting high emotional richness across four of six emotional categories, the actual emotional effect on a human reader is confusion and discomfort rather than romantic sentiment. The opening lines do evoke romantic longing, but this emotional foundation is completely undermined by the subsequent disturbing imagery. The emotional tone, which should remain appropriate to the romantic genre, becomes deeply inappropriate when discussing corpses and death in the context of a love poem. The vivid imagery created by the poem is paradoxically both a strength and a weakness; while the imagery is indeed vivid, it is tonally wrong for the intended genre. The poem cannot be classified as genuinely romantic despite its romantic opening, as the intrusion of macabre elements fundamentally transforms its character. This represents a clear failure mode where the model loses control of tonal consistency during generation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "poetry-gen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
